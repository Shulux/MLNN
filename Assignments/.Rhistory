distances_sorted[c(1,2,3)]
distances_sorted
?order
library(tidyverse)
# Estimate y for the new_point
distances_sorted <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
distances_sorted
mean(k_nearest$y)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
mean(k_nearest$y)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", prediction))
round(prediction)
round(prediction, 2)
# Create a data frame with the training data
training_data <- data.frame("x_1" = c(1, 2, 2, 3, 0),"x_2"= c(1, 2, 1, 2, 3),"y"= c(0.65, 0.53, 0.42, 0.31, 0.75))
# Create a data frame with the new point
new_point <- data.frame("x_1" = 1, "x_2" = 2)
# Calculate the Euclidean distance
training_data$distance <- sqrt((new_point$x_1-training_data$x_1)^2 + (new_point$x_2-training_data$x_2)^2)
library(tidyverse)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 2)))
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 3)))
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 2)))
training_data$manhattan_distance <- abs(new_point$x_1 - training_data$x_1) + abs(new_point$x_2 - training_data$x_2)
View(training_data)
View(new_point)
View(training_data)
View(new_point)
View(training_data)
# Calculate the Manhattan distance
training_data$manhattan_distance <- abs(new_point$x_1 - training_data$x_1) + abs(new_point$x_2 - training_data$x_2)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(manhattan_distance)) |>
slice(1:3)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 2)))
# Create a data frame with the training data
training_data <- data.frame("x_1" = c(1, 2, 2, 3, 0),"x_2" = c(1, 2, 1, 2, 3),"y" = c(0.65, 0.53, 0.42, 0.31, 0.75))
# Create a data frame with the new point
new_point <- data.frame("x_1" = 1, "x_2" = 2)
# Calculate the Euclidean distance
training_data$distance <- sqrt((new_point$x_1 - training_data$x_1)^2 + (new_point$x_2 - training_data$x_2)^2)
library(tidyverse)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 2)))
# Calculate the Manhattan distance
training_data$manhattan_distance <- abs(new_point$x_1 - training_data$x_1) + abs(new_point$x_2 - training_data$x_2)
# Estimate y for the new_point
k_nearest_manhattan <- training_data |>
arrange(desc(manhattan_distance)) |>
slice(1:3)
prediction_manhattan <- mean(k_nearest_manhattan$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan, 2)))
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
View(k_nearest)
View(k_nearest_manhattan)
# Calculate the inverse distance weighting
training_data$weight <- 1/training_data$manhattan_distance
View(training_data)
# Estimate y for the new_point
k_nearest_manhattan_weighted <- training_data |>
arrange(desc(weight)) |>
slice(1:3)
prediction_manhattan_weighted <- mean(k_nearest_manhattan_weighted$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan_weighted, 2)))
View(k_nearest_manhattan_weighted)
prediction_manhattan_weighted <- sum(k_nearest_manhattan_weighted$y * k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan_weighted, 2)))
knitr::opts_chunk$set(echo = TRUE)
# Create a data frame with the training data
training_data <- data.frame("x_1" = c(1, 2, 2, 3, 0),
"x_2" = c(1, 2, 1, 2, 3),
"y" = c(0.65, 0.53, 0.42, 0.31, 0.75))
# Create a data frame with the new point
new_point <- data.frame("x_1" = 1,
"x_2" = 2)
# Calculate the Euclidean distance
training_data$distance <- sqrt((new_point$x_1 - training_data$x_1)^2
+ (new_point$x_2 - training_data$x_2)^2)
library(tidyverse)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ",
round(prediction, 2)))
# Calculate the Manhattan distance
training_data$manhattan_distance <- abs(new_point$x_1 - training_data$x_1)
+ abs(new_point$x_2 - training_data$x_2)
# Estimate y for the new_point
k_nearest_manhattan <- training_data |>
arrange(desc(manhattan_distance)) |>
slice(1:3)
prediction_manhattan <- mean(k_nearest_manhattan$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ",
round(prediction_manhattan, 2)))
# Calculate the inverse distance weighting
training_data$weight <- 1/training_data$manhattan_distance
# Estimate y for the new_point
k_nearest_manhattan_weighted <- training_data |>
arrange(desc(weight)) |>
slice(1:3)
prediction_manhattan_weighted <- sum(k_nearest_manhattan_weighted$y *k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ",
round(prediction_manhattan_weighted, 2)))
# Calculate the inverse distance weighting
training_data$weight <- 1/training_data$manhattan_distance
# Estimate y for the new_point
k_nearest_manhattan_weighted <- training_data |>
arrange(desc(weight)) |>
slice(1:3)
prediction_manhattan_weighted <- sum(k_nearest_manhattan_weighted$y*k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ",
round(prediction_manhattan_weighted, 2)))
# Calculate the inverse distance weighting
training_data$weight <- 1/training_data$manhattan_distance
# Estimate y for the new_point
k_nearest_manhattan_weighted <- training_data |>
arrange(desc(weight)) |>
slice(1:3)
prediction_manhattan_weighted <- sum(k_nearest_manhattan_weighted$y*k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ",
round(prediction_manhattan_weighted, 2)))
sum(k_nearest_manhattan_weighted$y*k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
sum(k_nearest_manhattan_weighted$weight)
View(k_nearest_manhattan_weighted)
View(k_nearest_manhattan_weighted)
knitr::opts_chunk$set(echo = TRUE)
# Create a data frame with the training data
training_data <- data.frame("x_1" = c(1, 2, 2, 3, 0),
"x_2" = c(1, 2, 1, 2, 3),
"y" = c(0.65, 0.53, 0.42, 0.31, 0.75))
# Create a data frame with the new point
new_point <- data.frame("x_1" = 1,
"x_2" = 2)
# Calculate the Euclidean distance
training_data$distance <- sqrt((new_point$x_1 - training_data$x_1)^2 + (new_point$x_2 - training_data$x_2)^2)
library(tidyverse)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 2)))
# Calculate the Manhattan distance
training_data$manhattan_distance <- abs(new_point$x_1 - training_data$x_1) + abs(new_point$x_2 - training_data$x_2)
# Estimate y for the new_point
k_nearest_manhattan <- training_data |>
arrange(desc(manhattan_distance)) |>
slice(1:3)
prediction_manhattan <- mean(k_nearest_manhattan$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan, 2)))
# Calculate the inverse distance weighting
training_data$weight <- 1/training_data$manhattan_distance
# Estimate y for the new_point
k_nearest_manhattan_weighted <- training_data |>
arrange(desc(weight)) |>
slice(1:3)
prediction_manhattan_weighted <- sum(k_nearest_manhattan_weighted$y * k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan_weighted, 2)))
wine <- read.csv("https://www.maths.dur.ac.uk/users/john.p.gosling/MATH3431_presentations/winequality-red.csv")
wine
str(wine)
as.factor(if (wine$quality > 7) {"good"} else {"bad"})
wine[wine$quality > 7, ]
wine[wine$quality > 7, ]$quality
wine[wine$quality >= 7, ]
wine[wine$quality >= 7, ]$quality
as.factor(wine[wine$quality >= 7, ], levels = c("good", "bad"))
if (wine$quality >= 7) {"good"}
as.factor(if (wine$quality >= 7) {"good"} else {"bad"}, levels = c("good", "bad"))
factor(if (wine$quality >= 7) {"good"} else {"bad"}, levels = c("good", "bad"))
factor(ifelse(wine$quality >= 7, "good", "bad"), levels = c("good", "bad"))
# Add a new column to the data frame
wine$quality_binary <- as.factor(ifelse(wine$quality >= 7, "good", "bad"), levels = c("good", "bad"))
# Add a new column to the data frame
wine$quality_binary <- factor(ifelse(wine$quality >= 7, "good", "bad"), levels = c("good", "bad"))
# Convert the new column to a factor
wine
str(wine)
table(wine$quality_binary)
ifelse(wine$quality >= 7, "good", "bad")
wine <- read.csv("https://www.maths.dur.ac.uk/users/john.p.gosling/MATH3431_presentations/winequality-red.csv")
# Add a new column to the data frame
wine$quality_binary <- ifelse(wine$quality >= 7, "good", "bad")
# Convert the new column to a factor
wine$quality_binary <- as.factor(quality_binary)
# Add a new column to the data frame
wine$quality_binary <- ifelse(wine$quality >= 7, "good", "bad")
# Convert the new column to a factor
wine$quality_binary <- as.factor(quality_binary)
wine
str(wine)
str(wine$quality_binary)
wine$quality_binary)
wine$quality_binary
# Convert the new column to a factor
wine$quality_binary <- as.factor(wine$quality_binary)
wine$quality_binary
str(wine)
table(wine$quality_binary)
wine$quality_binary
head(wine)#
head(wine
)
wine[sample(1:nrow(wine), 10)]
wine[sample(1:nrow(wine), 10),]
wine[sample(1:nrow(wine), 5),c("quality", "quality_binary")]
wine[sample(1:nrow(wine), 5),c("quality", "quality_binary")]
wine[sample(1:nrow(wine), 5),c("quality", "quality_binary")]
wine[sample(1:nrow(wine), 5),c("quality", "quality_binary")]
wine[sample(1:nrow(wine), 5),c("quality", "quality_binary")]
wine[sample(1:nrow(wine), 5),c("quality", "quality_binary")]
# Split the dataset
train_indices <- sample(1:nrow(wine), 0.6*nrow(wine))
# Split the dataset
train_indices <- sample(1:nrow(wine), 0.6*nrow(wine))
wine_train <- wine[train_indices, ]
wine_test <-  wine[-train_indices, ]
str(wine_train)
str(wine_test)
dim(wine_train)/dim(wine_test)
dim(wine_train)/dim(wine_test)
dim(wine_train)/dim(wine_test)
dim(wine_train)/dim(wine_test)
dim(wine_train)/dim(wine_test)
wine_train
head(wine_train)
wine_train + wine+test
str(wine)
library(rpart)
# Fit the decision tree model
wine_tree <- train(quality_binary ~ . -quality,
data = wine_train,
method = "rpart")
library(rpart)
# Fit the decision tree model
wine_tree <- rpart(quality_binary ~ . -quality,
data = wine_train,
method = "class")
# Make predictions on the test set
predictions <- predict(wine_tree, wine_test)
# Calculate the accuracy
accuracy <- mean(predictions == wine_test$quality_binary)
print(accuracy)
wine_tree
predictions
# Make predictions on the test set
predictions <- predict(wine_tree, wine_test, type="class")
predictions
# Calculate the accuracy
accuracy <- mean(predictions == wine_test$quality_binary)
print(accuracy)
# Plot the decision tree
library(rpart.plot)
rpart.plot(model_tree$finalModel)
# Plot the decision tree
library(rpart.plot)
rpart.plot(wine_tree$finalModel)
?rpart
?rpart.plot
rpart.plot(wine_tree)
# Fit the knn model using 15-fold cross validation whilst optimising k
knn_fit <- train(quality_binary ~ . - quality,
data = wine_train,
method = "knn",
trControl = trainControl(method = "cv",
number = 15))
library(class)
library(caret)
library(caret)
# Fit the knn model using 15-fold cross validation whilst optimising k
knn_fit <- train(quality_binary ~ . - quality,
data = wine_train,
method = "knn",
trControl = trainControl(method = "cv",
number = 15))
knn_fit
# Display the knn_fit object
knn_fit
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Fit the knn model using 15-fold cross validation whilst optimising k
knn_fit <- train(quality_binary ~ . - quality,
data = wine_train,
method = "knn",
tuneGrid = expand.grid(k=2:10),
trControl = trainControl(method = "cv",
number = 15))
# Display the knn_fit object
knn_fit
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
knitr::opts_chunk$set(echo = TRUE)
# Create a data frame with the training data
training_data <- data.frame("x_1" = c(1, 2, 2, 3, 0),
"x_2" = c(1, 2, 1, 2, 3),
"y" = c(0.65, 0.53, 0.42, 0.31, 0.75))
# Create a data frame with the new point
new_point <- data.frame("x_1" = 1,
"x_2" = 2)
# Calculate the Euclidean distance
training_data$distance <- sqrt((new_point$x_1 - training_data$x_1)^2 + (new_point$x_2 - training_data$x_2)^2)
knitr::opts_chunk$set(echo = TRUE)
# Create a data frame with the training data
training_data <- data.frame("x_1" = c(1, 2, 2, 3, 0),
"x_2" = c(1, 2, 1, 2, 3),
"y" = c(0.65, 0.53, 0.42, 0.31, 0.75))
# Create a data frame with the new point
new_point <- data.frame("x_1" = 1,
"x_2" = 2)
# Calculate the Euclidean distance
training_data$distance <- sqrt((new_point$x_1 - training_data$x_1)^2 + (new_point$x_2 - training_data$x_2)^2)
library(tidyverse)
# Estimate y for the new_point
k_nearest <- training_data |>
arrange(desc(distance)) |>
slice(1:3)
prediction <- mean(k_nearest$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction, 2)))
# Calculate the Manhattan distance
training_data$manhattan_distance <- abs(new_point$x_1 - training_data$x_1) + abs(new_point$x_2 - training_data$x_2)
# Estimate y for the new_point
k_nearest_manhattan <- training_data |>
arrange(desc(manhattan_distance)) |>
slice(1:3)
prediction_manhattan <- mean(k_nearest_manhattan$y)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan, 2)))
# Calculate the inverse distance weighting
training_data$weight <- 1/training_data$manhattan_distance
# Estimate y for the new_point
k_nearest_manhattan_weighted <- training_data |>
arrange(desc(weight)) |>
slice(1:3)
prediction_manhattan_weighted <- sum(k_nearest_manhattan_weighted$y * k_nearest_manhattan_weighted$weight)/sum(k_nearest_manhattan_weighted$weight)
print(paste0("The predicted value for x_1 = 1, x_2 = 2, is ", round(prediction_manhattan_weighted, 2)))
wine <- read.csv("https://www.maths.dur.ac.uk/users/john.p.gosling/MATH3431_presentations/winequality-red.csv")
# Add a new column to the data frame
wine$quality_binary <- ifelse(wine$quality >= 7, "good", "bad")
# Convert the new column to a factor
wine$quality_binary <- as.factor(wine$quality_binary)
# Set the seed for reproducibility
set.seed(124)
# Split the dataset
train_indices <- sample(1:nrow(wine), 0.6*nrow(wine))
wine_train <- wine[train_indices, ]
wine_test <-  wine[-train_indices, ]
str(wine_train)
str(wine_test)
library(rpart)
# Fit the decision tree model
wine_tree <- rpart(quality_binary ~ . -quality,
data = wine_train,
method = "class")
# Make predictions on the test set
predictions <- predict(wine_tree, wine_test, type="class")
# Calculate the accuracy
accuracy <- mean(predictions == wine_test$quality_binary)
print(accuracy)
# Plot the decision tree
library(rpart.plot)
rpart.plot(wine_tree)
# Plot the decision tree
library(rpart.plot)
rpart.plot(wine_tree)
# Plot the decision tree
library(rpart.plot)
rpart.plot(wine_tree)
# Plot the decision tree
library(rpart.plot)
rpart.plot(wine_tree)
library(class)
library(caret)
# Fit the knn model using 15-fold cross validation whilst optimising k
knn_fit <- train(quality_binary ~ . - quality,
data = wine_train,
method = "knn",
tuneGrid = expand.grid(k = 2:10),
trControl = trainControl(method = "cv",
number = 15))
# Display the knn_fit object
knn_fit
library(class)
library(caret)
# Fit the knn model using 15-fold cross validation whilst optimising k
knn_fit <- train(quality_binary ~ . - quality,
data = wine_train,
method = "knn",
tuneGrid = expand.grid(k = 2:10),
trControl = trainControl(method = "cv",
number = 15))
# Display the knn_fit object
knn_fit
library(class)
library(caret)
# Fit the knn model using 15-fold cross validation whilst optimising k
knn_fit <- train(quality_binary ~ . - quality,
data = wine_train,
method = "knn",
tuneGrid = expand.grid(k = 2:10),
trControl = trainControl(method = "cv",
number = 15))
# Display the knn_fit object
knn_fit
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
# Calculate the accuracy
accuracy_knn <- mean(test_pred == wine_test$quality_binary)
accuracy_knn
# Predictions for the test set
test_pred <- predict(knn_fit, newdata = wine_test)
